# -*- coding: utf-8 -*-
import os
import re
import json
import logging
import hashlib
import subprocess
from operator import attrgetter
from itertools import islice
from os.path import basename

from Bio import SeqIO
from bunch import Bunch
from toolz import groupby
from toolz import second
from toolz import countby

import cutlass
import cutlass.Project
from cutlass import iHMPSession
from cutlass.mixs import MIXS

from anadama.util import fname
from anadama.util import HasNoEqual

from all_pipeline import output_dirs
from all_pipeline import metadata_json
from package_dcc_seqs import output_dirs as tar_dirs

conf = Bunch(
    session = Bunch(user = "rschwager", passwd = r"peqTyQ1O4eajGWpPudrwqYtsrP50lSqR"),
    project = Bunch( name = "iHMP" ),
    study   = Bunch( 
        name = "ibdmdb", # TODO: remove test when ready to go live
        description = """A new three-year "multi-omic" study investigating the roles played by microbes and their interactions with the human body.""",
        contact = "Randall Schwager <schwager@hsph.harvard.edu>",
        center = "Broad Institute",
        subtype = "ibd",
        mixs = Bunch( biome = "Human, gut",
                      project_name = "Inflammatory Bowel Disease Multi-omics Database (IBDMDB)" ),
        ),
    sample  = Bunch(
        water = "ENVO:00005791", # sterile water
        ethanol = "ethanol",
        biome = "ENVO:00009003",
        feature = "ENVO:00002003", # Feces
        geo_loc_name = "USA",
        lat_lon = "+42.363664 -71.069230",
        body_product = "stool",
        env_package = "Human-associated",
        fma_body_site = "FMA:64183", # Feces
        body_site = "stool",
        ),
    sixt    = Bunch(
        prep_id = "1",
        library_method_text = \
            """ Samples are diluted and run in a 30 cycle PCR that specifically
            amplifies the V4 region of the 16S ribosomal subunit, with the
            primers making use of the highly conserved regions on either
            side. PCR primers are barcoded by well, in a dual plate format,
            for a total of 192 available barcodes. Samples are quantified
            using the caliper, and then pooled together. Finally, the pooled
            sample is cleaned up and selected for size by using a combination
            of purification columns and a Pippen Prep SAGE protocol, and
            handed off for MiSeq. """,
        ),
    wgs     = Bunch(
        library_method_text = \
        """Metagenomic DNA samples were quantified by Quant-iT PicoGreen dsDNA
        Assay (Life Technologies) and normalized to a concentration of
        50 pg/microliter. Illumina sequencing libraries were prepared from
        100-250 pg DNA using the Nextera XT DNA Library Preparation
        kit (Illumina) according to the manufacturer's recommended
        protocol, with reaction volumes scaled accordingly. Batches of
        24, 48, or 96 libraries were pooled by transferring equal
        volumes of each library using a Labcyte Echo 550 liquid
        handler. Insert sizes and concentrations for each pooled
        library were determined using an Agilent Bioanalyzer DNA 1000
        kit (Agilent Technologies).""",
        prep_id = "2",
        sequencing_contact = "tpoon@broadintitute.org",
        sequencing_center = "Broad Institute",
        ncbi_taxon_id = "408170", # human gut metagenomes
        lib_layout = "fragment",
        lib_selection = "Random",
        prep_comment = "Broad IBDMDB default WGS dna prep",
        seq_comment = "Raw WGS Sequence set ",
        storage_duration = 365, # 1 year in days
        sequencer_model = "Illumina HiSeq 2000",
        ),
    mtx     = Bunch(
        library_method_text = \
        """ Our method, RNAtag-Seq, utilizes a molecular barcoding strategy
        that generates multiple libraries in a single reaction, lowers
        labor and reagent costs per sample, and produces data on
        prokaryotic and eukaryotic samples that are comparable to
        those generated by traditional strand-specific RNA-seq
        approaches.  Barcoding in RNAtag-Seq is achieved through
        direct ligation of adaptors to RNA, enabling high quality,
        strand-specific, quantitative sequencing of full-length
        transcripts in diverse prokaryotic and eukaryotic species.
        """,
        prep_id = "3",
        sequencing_contact = "tpoon@broadintitute.org",
        sequencing_center = "Broad Institute",
        ncbi_taxon_id = "408170", # human gut metagenomes
        lib_layout = "fragment",
        lib_selection = "Random",
        prep_comment = "Broad IBDMDB default Microbe Transcriptomics dna prep",
        seq_comment = "Raw Microbe Transcriptomics Sequence set ",
        storage_duration = 365, # 1 year in days
        sequencer_model = "Illumina HiSeq 2000",
        ),
    prot    = Bunch(
        prep_id = "4",
        pride_id = "",
        ),
)


racemap = {
    '1': "american_indian_or_alaska_native",
    '2': "asian",
    '3': "american_indian_or_alaska_native",
    '4': "african_american",
    '5': "caucasian",
    '6': "ethnic_other",
    '7': "ethnic_other",
    '8': "ethnic_other"
}

sexmap = {
    "0": "female",
    "1": "male",
    None: "unknown",
}

dxmap = {
    "0": "crohns_disease",
    "1": "ulcerative_colitis",
    "2": "healthy_control",
    "3": "other",
}

dietmap = {
    "sugar_drinks": 'dr_q10',
    "juice": 'dr_q12',
    "water": 'dr_q13',
    "alcohol": 'dr_q14',
    "yogurt": 'dr_q15',
    "dairy": 'dr_q16', 
    "probiotic": 'dr_q17',
    "fruit": 'dr_q18',
    "veg": 'dr_q19',
    "beans": 'dr_q20',
    "grains": 'dr_q21',
    "starch": 'dr_q22',
    "eggs": 'dr_q23',
    "meat_product": 'dr_q24',
    "meat_red": 'dr_q25',
    "meat_white": 'dr_q26',
    "shellfish": u'dr_q27',
    "fish": 'dr_q28',
    "sweets": u'dr_q29',
}

boolmap = {
    "no":    False,
    "n":     False,
    "false": False,
    "f":     False,
    "null":  False,
    "nan":   False,
    "nil":   False,
    "n/a":   False,

    "yes":   True,
    "y":     True,
    "t":     True,
    "true":  True,
}

neq = HasNoEqual()

limsconvert = lambda k: k[1:3]+"-"+k[3:]

getext = lambda s: re.sub(r'.+?(\..*)', r'\1', s)
_gettermap = {}
def setup_getters(headers):
    global _gettermap
    for i, val in enumerate(headers):
        _gettermap[val] = i

def get(row, key):
    return row[ _gettermap[key] ]

def getbool(row, key):
    val = get(row, key)
    if not val:
        return False
    try:
        ret = int(val)
    except ValueError:
        ret = boolmap.get(val.lower(), True)
    return bool(ret)

def textsanitize(t):
    return re.sub(r'\s+', ' ', t)


def default_mixs_dict():
    return dict([ (k, v()) for k, v in MIXS._fields.iteritems() ])


def default_mimarks_dict():
    return dict([ (k, v()) for k, v in
                  cutlass.mimarks.MIMARKS._fields.iteritems() ])


def default_mims_dict():
    return dict([ (k, v()) for k, v in
                  cutlass.mims.MIMS._fields.iteritems() ])


def project(session):
    res = session.get_osdf().oql_query(
        "ihmp", '"project"[node_type]'
    )['results']
    for data in res:
        if data['meta']['name'] == conf.project.name:
            return cutlass.Project.load_project(data)
    raise Exception("Unable to find project "+conf.project.name)


def update(obj, key, value):
    prev = getattr(obj, key)
    if value == prev:
        return False
    setattr(obj, key, value)
    logging.info("Setting dirty on %s: key `%s' was `%s' is now `%s'",
                 obj, key, prev, value)
    return True

def update_tag(obj, key, value):
    d = dict([ t.split(":") for t in obj.tags if ":" in t])
    if key not in d or (key in d and d[key] != value):
        d[key] = value
        othertags = [t for t in obj.tags if ":" not in t]
        obj.tags = othertags + [ str(k)+":"+(v) for k,v in d.iteritems() ]
        logging.info("Setting dirty on %s: new tag `%s'", obj, key+":"+value)
        return True
    else:
        return False


def update_item(obj, key, value):
    if key not in obj:
        obj[key] = value
        logging.info("Setting dirty on %s: new dict item `%s'", obj, key+":"+value)
        return True
    prev = obj[key]
    if value == prev:
        return False
    logging.info("Setting dirty on %s: key `%s' was `%s' is now `%s'",
                 obj, key, prev, value)
    obj[key] = value
    return True


def update_file(obj, filename, md5sum):
    if not any( basename(filename) in u for u in obj.urls ):
        obj.local_file = filename
        logging.info("Setting dirty on %s: `%s' not found in urls", 
                     obj, basename(filename))
        return True
    elif obj.checksums.get('md5', neq) != md5sum:
        obj.local_file = filename
        logging.info("Setting dirty on %s: md5sum different `%s' != `%s'",
                     obj, obj.checksums.get('md5', neq), md5sum)
        return True
    obj.local_file = filename # used later, but no need to save to dcc
    return False
        


def save(obj, use_encryption=None):
    kwargs = {}
    if use_encryption is not None:
        kwargs['use_ascp_encryption'] = use_encryption
    if getattr(obj, "_dirty", True) or bool(getattr(obj, "id", False)) is False:
        try:
            ret = obj.save(**kwargs)
        except Exception as e:
            if "Version provided" in str(e):
                match = re.compile(r'match saved \((\d+)\)').search(str(e))
                right_ver = match.group(1)
                obj.version = int(right_ver)
                ret = obj.save()
                logging.debug("Successfully saved obj %s", obj)
            else:
                raise
        if ret is False:
            problems = obj.validate()
            import pdb; pdb.set_trace()
            logging.error("Unable to save %s due to errors %s", 
                          obj, problems)
    if hasattr(obj, "mysave"):
        obj.oldsave = obj.save
        obj.save = obj.mysave
        del obj.oldsave


def study(session, pr):
    res = session.get_osdf().oql_query(
        "ihmp", '"study"[node_type]'
    )['results']
    st = None
    for data in res:
        if data['meta']['name'] == conf.study.name:
            st = cutlass.Study.load_study(data)
            break
    if not st:
        raise Exception("Unable to find project "+conf.study.name)
    dirty = False
    dirty |= update(st, "name", conf.study.name)
    dirty |= update(st, "description", conf.study.description)
    dirty |= update(st, "contact", conf.study.contact)
    dirty |= update(st, "center", conf.study.center)
    dirty |= update(st, "subtype", conf.study.subtype)
    st.mixs = d = default_mixs_dict()
    d.update(dict(conf.study.mixs))
    st.links['part_of'] = pr.id
    st._dirty = dirty
    return st
    

def aggregate(json_fname):
    with open(json_fname, 'r') as f:
        data = Bunch(json.load(f))
    setup_getters(data['headers'])
    subjs = groupby(2, data['data'])
    for k in subjs:
        subjs[k] = groupby(7, subjs[k])
    return data, subjs


def sync_subject(st, maybe_dcc, row):
    dirty = False
    if not maybe_dcc:
        subj = cutlass.Subject()
    else:
        subj = maybe_dcc[0]
    dirty |= update(subj, "rand_subject_id", str(re.sub(r'\D+', '', row[2])))
    dirty |= update(subj, "gender", sexmap.get(get(row, "sex"), sexmap[None]))
    dirty |= update(subj, "race", racemap.get(get(row, "race"), racemap['8']))
    dirty |= update_tag(subj, "diagnosis", dxmap.get(get(row, "diagnosis"), "other"))
    dirty |= update_tag(subj, "age_at_dx", get(row, "age_dx"))
    dirty |= update_tag(subj, "highest_education", get(row, 'b_q66'))
    save(st)
    subj.links['participates_in'] = [st.id]
    subj._dirty = dirty
    return subj
    

def sync_visit(maybe_dcc, subj, row):
    dirty = False
    if not maybe_dcc:
        v = cutlass.Visit()
        va = cutlass.VisitAttribute()
    else:
        v = maybe_dcc[0]
        vas = list(v.attributes())
        if not vas:
            va = cutlass.VisitAttribute()
        else:
            va = vas[0]

    def _s():
        ret = v.mysave()
        va.links['associated_with'] = [v.id]
        va.save()
        return ret
    v.mysave = v.save
    v.save = _s

    dirty |= update(v, "visit_id", "{}_{}".format(get(row, 'ProjectSpecificID'),
                                                 get(row, "visit_num")))
    dirty |= update(v, "visit_number", get(row, "visit_num"))
    dirty |= update(v, "date", "2000-01-01")
    dirty |= update(v, "interval", get(row, "interval_days"))
    dirty |= update(va, "hbi_total", float(get(row, 'hbi_score') or 0))
    dirty |= update(va, "abx", getbool(row, "dr_q2a"))
    dirty |= update(va, "chemo", getbool(row, "dr_q2b"))
    for key,val in dietmap.items():
        dirty |= update(va, key, getbool(row, val))
    save(subj)
    v.links['by'] = [subj.id]
    v._dirty = dirty
    return v


def sync_sample(maybe_dcc, visit, row):
    dirty = False
    if not maybe_dcc:
        samp = cutlass.Sample()
        sa = cutlass.SampleAttribute()
    else:
        samp = maybe_dcc[0]
        sas = list(samp.sampleAttributes())
        if not sas:
            sa = cutlass.SampleAttribute()
        else:
            sa = sas[0]

    def _s():
        ret = samp.mysave()
        sa.links['associated_with'] = [samp.id]
        sa.save()
        return ret
    samp.mysave = samp.save
    samp.save = _s

    lims_label = limsconvert(row[1])
    d = samp.mixs or default_mixs_dict()
    dirty |= update_item(d, "biome", conf.sample.biome)
    dirty |= update_item(d, "collection_date", "2000-01-01")
    matl = conf.sample.water if get(row, "st_q5") == lims_label else conf.sample.ethanol
    dirty |= update_item(d, "material", matl)
    dirty |= update_item(d, "env_package", conf.sample.env_package)
    dirty |= update_item(d, "geo_loc_name", conf.sample.geo_loc_name)
    dirty |= update_item(d, "lat_lon", conf.sample.lat_lon)
    dirty |= update_item(d, "body_product", conf.sample.body_product)
    samp.mixs = d
    dirty |= update(sa, "fecalcal", str(get(row, 'st_q16')))
    dirty |= update(sa, "study", "ibd")
    dirty |= update(samp, "fma_body_site", conf.sample.fma_body_site)
    dirty |= update(samp, "body_site", conf.sample.body_site)
    dirty |= update(samp, "name", str(lims_label))
    save(visit)
    samp.links['collected_during'] = [visit.id]
    samp._dirty = dirty
    return samp


def dccify(session, st, agg):
    subjs, visits, samples = [], [], []
    dcc_subs = groupby(lambda s: int(s.rand_subject_id), st.subjects())
    for subj_id, grp in agg.iteritems():
        subj_id = re.sub(r'\D+', '', subj_id)
        maybe_dcc_subj = dcc_subs.get(subj_id, [])
        if len(maybe_dcc_subj) > 1:
            raise Exception("Strange subject ID refers to "
                            "multiple subjects: "+subj_id )
        subj = sync_subject(st, maybe_dcc_subj, next(grp.itervalues())[0])
        subjs.append(subj)
        dcc_visits = groupby(attrgetter("visit_number"), subj.visits())
        for visit_no, sample_group in grp.iteritems():
            if visit_no is None:
                logging.warning("Unable to save visits or samples for subject"
                                " %s because it has no visit numbers", subj_id)
                continue
            dcc_visit = dcc_visits.get(int(visit_no), [])
            dcc_visit = sync_visit(dcc_visit, subj, sample_group[0])
            visits.append(dcc_visit)
            dcc_samples = groupby(attrgetter("name"), dcc_visit.samples())
            for sample in sample_group:
                k = sample[1]
                maybe_dcc_samp = dcc_samples.get(limsconvert(k), [])
                samples.append( sync_sample(maybe_dcc_samp, dcc_visit, sample) )
    return subjs, visits, samples


def seq_len_mode(fname, sample_size=200):
    """Return the most common length of sequences from a given sequence
    file"""

    with open(os.devnull, 'w') as dn:
        proc = subprocess.Popen(
            "tar -xvOf {} | gzip -d".format(fname),
            shell=True, stdout=subprocess.PIPE, stderr=dn)
        seq_sample = islice( SeqIO.parse(proc.stdout, "fastq"),
                             None, sample_size )
        try:
            ret = max(countby(len, seq_sample).iteritems(), key=second)[0]
        except ValueError:
            ret = 0
        proc.stdout.close()
        proc.terminate()
        del proc
    return ret

def find_sumfile(fn):
    if os.path.exists(fn+".md5"):
        return fn+".md5"
    elif os.path.exists(fname.mangle(fn, ext="md5")):
        return fname.mangle(fn, ext="md5")


def md5(fname):
    maybe_sumfile = find_sumfile(fname)
    if maybe_sumfile:
        with open(maybe_sumfile, 'r') as f:
            sm = f.read().strip().split()[0]
    else:
        sm = _md5(fname)
        with open(fname+".md5", 'w') as f:
            print >> f, sm
    return sm


md5_buffer_len = 4096
def _md5(fname):
    m = hashlib.md5()
    with open(fname, 'rb') as f:
        while True:
            buf = f.read(md5_buffer_len)
            if not buf:
                break
            m.update(buf)
    return m.hexdigest()


def save_16s(samp, row):
    maybe_prep = list(samp.sixteenSDnaPreps())
    if maybe_prep:
        prep = maybe_prep[0]
    else:
        prep = cutlass.SixteenSDnaPrep()
    dirty = False
    dirty |= update(prep, "comment", "Broad IBDMDB default 16S dna prep")
    dirty |= update(prep, "ncbi_taxon_id", "408170")
    dirty |= update(prep, "lib_selection", "Random")
    dirty |= update(prep, "lib_layout", "fragment")
    dirty |= update(prep, "prep_id", conf.sixt.prep_id)
    d = prep.mimarks or default_mimarks_dict()
    dirty |= update_item(d, "target_gene", "16S")
    dirty |= update_item(d, "lib_const_meth", textsanitize(conf.sixt.library_method_text))
    dirty |= update_item(d, "target_subfragment", "V4")
    if not prep.mimarks:
        prep.mimarks = d
    dirty |= update(prep, "sequencing_center", "Broad Institute")
    dirty |= update(prep, "sequencing_contact", "tpoon@broadinstitute.org")
    dirty |= update(prep, "storage_duration", 365)
    prep._dirty = dirty
    prep.links['prepared_from'] = [samp.id]
    save(prep)

    seqs = list(prep.raw_seq_sets())
    if seqs:
        seq = seqs[0]
    else:
        seq = cutlass.SixteenSRawSeqSet()
    dirty = False
    dirty |= update(seq, "comment", str("Raw, untrimmed 16S sample "+row[1]))
    fn = str(tar_dirs.amplicon+"/"+row[1]+".tar")
    md5sum = md5(fn)
    dirty |= update_file(seq, fn, md5sum)
    dirty |= update(seq, "checksums", {"md5": md5sum})
    dirty |= update(seq, "exp_length", seq_len_mode(seq.local_file))
    dirty |= update(seq, "format", "fastq")
    dirty |= update(seq, "format_doc", "https://en.wikipedia.org/wiki/FASTQ_format")
    dirty |= update(seq, "seq_model", "Illumina MiSeq")
    dirty |= update(seq, "size", os.stat(seq.local_file).st_size)
    dirty |= update(seq, "study", "ibd")
    seq._dirty = dirty
    seq.links['sequenced_from'] = [prep.id]
    save(seq, True)

    trms = list(seq.trimmed_seq_sets())
    if trms:
        trm = trms[0]
    else:
        trm = cutlass.SixteenSTrimmedSeqSet()
    dirty = False
    dirty |= update(trm, "comment", str("Trimmed 16S sample "+row[1]))
    dirty |= update_file(trm, seq.local_file, md5sum)
    dirty |= update(trm, "checksums", {"md5": md5sum})
    dirty |= update(trm, "format", "fastq")
    dirty |= update(trm, "format_doc", "https://en.wikipedia.org/wiki/FASTQ_format")
    dirty |= update(trm, "size", os.stat(seq.local_file).st_size)
    dirty |= update(trm, "study", "ibd")
    trm._dirty = dirty
    trm.links['computed_from'] = [seq.id]
    save(trm, True)

    abd_fname = output_dirs.amplicon.taxprof+"/"+row[1]+".biom"
    if not os.path.exists(abd_fname):
        return

    mtxs = list(trm.abundance_matrices())
    if mtxs:
        mtx = mtxs[0]
    else:
        mtx = cutlass.AbundanceMatrix()
    dirty = False
    md5sum = md5(abd_fname)
    dirty |= update_file(mtx, str(abd_fname), md5sum)
    dirty |= update(mtx, "checksums", {"md5": md5sum})    
    dirty |= update(mtx, "format", "biom")
    dirty |= update(mtx, "format_doc", "http://biom-format.org/#Biomformat-1.3.1 ")
    dirty |= update(mtx, "matrix_type", "16s_community")
    dirty |= update(mtx, "size", os.stat(mtx.local_file).st_size)
    dirty |= update(mtx, "study", "ibd")
    dirty |= update(mtx, "comment", "16S taxonomic profile "+str(row[1]))
    
    mtx.links['computed_from'] = [trm.id]
    save(mtx, True)
        
    
def save_wgs(samp, row):
    maybe_prep = list(samp.wgsDnaPreps())
    if maybe_prep:
        prep = maybe_prep[0]
    else:
        prep = cutlass.WgsDnaPrep()
    dirty = False
    dirty |= update(prep, "comment", conf.wgs.prep_comment)
    dirty |= update(prep, "ncbi_taxon_id", conf.wgs.ncbi_taxon_id)
    dirty |= update(prep, "lib_selection", conf.wgs.lib_selection)
    dirty |= update(prep, "lib_layout", conf.wgs.lib_layout)
    dirty |= update(prep, "prep_id", conf.wgs.prep_id)
    d = prep.mims or default_mims_dict()
    dirty |= update_item(d, "lib_const_meth", textsanitize(conf.wgs.library_method_text))
    if not prep.mims:
        prep.mims = d
    dirty |= update(prep, "sequencing_center", conf.wgs.sequencing_center)
    dirty |= update(prep, "sequencing_contact", conf.wgs.sequencing_contact)
    dirty |= update(prep, "storage_duration", conf.wgs.storage_duration)
    prep._dirty = dirty
    prep.links['prepared_from'] = [samp.id]
    save(prep)

    seqs = list(prep.child_seq_sets(types=["wgs_raw_seq_set"]))
    if seqs:
        seq = seqs[0]
    else:
        seq = cutlass.WgsRawSeqSet()
    dirty = False
    dirty |= update(seq, "comment", str(conf.wgs.seq_comment+row[1]))
    fn = str(tar_dirs.metagenomics+"/"+row[1]+".tar")
    md5sum = md5(fn)
    dirty |= update_file(seq, fn, md5sum)
    dirty |= update(seq, "checksums", {"md5": md5sum})
    dirty |= update(seq, "exp_length", seq_len_mode(seq.local_file))
    dirty |= update(seq, "format", "fastq")
    dirty |= update(seq, "format_doc", "https://en.wikipedia.org/wiki/FASTQ_format")
    dirty |= update(seq, "seq_model", conf.wgs.sequencer_model)
    dirty |= update(seq, "size", os.stat(seq.local_file).st_size)
    dirty |= update(seq, "study", "ibd")
    seq._dirty = dirty
    seq.links['sequenced_from'] = [prep.id]
    save(seq, True)

    abd_fname = output_dirs.metagenomics.taxprof+"/"+row[1]+".biom"
    if not os.path.exists(abd_fname):
        return

    mtxs = list(seq.abundance_matrices())
    if mtxs:
        mtx = mtxs[0]
    else:
        mtx = cutlass.AbundanceMatrix()
    dirty = False
    md5sum = md5(abd_fname)
    dirty |= update_file(mtx, str(abd_fname), md5sum)
    dirty |= update(mtx, "checksums", {"md5": md5sum})
    dirty |= update(mtx, "format", "biom")
    dirty |= update(mtx, "format_doc", "http://biom-format.org/#Biomformat-1.3.1 ")
    dirty |= update(mtx, "matrix_type", "wgs_community")
    dirty |= update(mtx, "size", os.stat(mtx.local_file).st_size)
    dirty |= update(mtx, "study", "ibd")
    dirty |= update(mtx, "comment", "WGS taxonomic profile "+str(row[1]))
    mtx.links['computed_from'] = [seq.id]
    save(mtx, True)
    

def save_mtx(samp, row):
    maybe_prep = list(samp.wgsDnaPreps())
    if maybe_prep:
        prep = maybe_prep[0]
    else:
        prep = cutlass.WgsDnaPrep()
    dirty = False
    dirty |= update(prep, "comment", conf.mtx.prep_comment)
    dirty |= update(prep, "ncbi_taxon_id", conf.mtx.ncbi_taxon_id)
    dirty |= update(prep, "lib_selection", conf.mtx.lib_selection)
    dirty |= update(prep, "lib_layout", conf.mtx.lib_layout)
    dirty |= update(prep, "prep_id", conf.mtx.prep_id)
    d = prep.mims or default_mims_dict()
    dirty |= update_item(d, "lib_const_meth", textsanitize(conf.mtx.library_method_text))
    if not prep.mims:
        prep.mims = d
    dirty |= update(prep, "sequencing_center", conf.mtx.sequencing_center)
    dirty |= update(prep, "sequencing_contact", conf.mtx.sequencing_contact)
    dirty |= update(prep, "storage_duration", conf.mtx.storage_duration)
    prep._dirty = dirty
    prep.links['prepared_from'] = [samp.id]
    save(prep)

    seqs = list(prep.child_seq_sets(types=["microb_transcriptomics_raw_seq_set"]))
    if seqs:
        seq = seqs[0]
    else:
        seq = cutlass.MicrobTranscriptomicsRawSeqSet()
    dirty = False
    dirty |= update(seq, "comment", str(conf.mtx.seq_comment+row[1]))
    fn = str(tar_dirs.metatranscriptomics+"/"+row[1]+".tar")
    md5sum = md5(fn)
    dirty |= update_file(seq, fn, md5sum)
    dirty |= update(seq, "checksums", {"md5": md5sum})
    dirty |= update(seq, "exp_length", seq_len_mode(seq.local_file))
    dirty |= update(seq, "format", "fastq")
    dirty |= update(seq, "format_doc", "https://en.wikipedia.org/wiki/FASTQ_format")
    dirty |= update(seq, "sequence_type", "nucleotide")
    dirty |= update(seq, "seq_model", str(conf.mtx.sequencer_model))
    dirty |= update(seq, "size", os.stat(seq.local_file).st_size)
    dirty |= update(seq, "study", "ibd")
    seq._dirty = dirty
    seq.links['sequenced_from'] = [prep.id]
    save(seq, True)

    
def save_proteomics(samp, row):
    maybe_prep = list(samp.microbiomeAssayPreps())
    if maybe_prep:
        prep = maybe_prep[0]
    else:
        prep = cutlass.MicrobiomeAssayPrep()
    dirty = False
    dirty |= update(prep, "comment", "Broad IBDMDB default proteomics prep")
    dirty |= update(prep, "pride_id", conf.prot.pride_id)
    dirty |= update(prep, "center", "pnnl")
    dirty |= update(prep, "sample_name", row[1])
    dirty |= update(prep, "contact", "White, Richard A <richard.white@pnnl.gov")
    dirty |= update(prep, "prep_id", conf.prot.prep_id)
    dirty |= update(prep, "storage_duration", 365)
    dirty |= update(prep, "epxeriment_type", "Shotgun Proteomics")
    dirty |= update(prep, "study", "ibd")
    prep._dirty = dirty
    prep.links['prepared_from'] = [samp.id]
    save(prep)

    seqs = list(prep.proteomes())
    if seqs:
        seq = seqs[0]
    else:
        seq = cutlass.Proteome()
    dirty = False
    fn = str(row[0])
    md5sum = md5(fn)
    dirty |= update(seq, "analyzer", "")
    dirty |= update(seq, "checksums", {"md5": md5sum})
    dirty |= update(seq, "comment", str("Raw, proteomics image: "+row[1]))
    dirty |= update(seq, "detector", "")
    dirty |= update(seq, "instrument_name", "")
    dirty |= update(seq, "pepid_format", "")
    dirty |= update(seq, "pepid_url", [""])
    dirty |= update(seq, "pride_id", "")
    dirty |= update(seq, "processing_method", "")
    dirty |= update(seq, "protid_format", "")
    dirty |= update(seq, "protmod_format", "")
    dirty |= update(seq, "protid_url", [""])
    dirty |= update_file(seq, fn, md5sum, name="local_protmod_file")
    dirty |= update(seq, "sample_name", row[1])
    dirty |= update(seq, "search_engine", "")
    dirty |= update(seq, "short_label", row[1])
    dirty |= update(seq, "software", "")
    dirty |= update(seq, "source", "")
    dirty |= update(seq, "spectra_format", "")
    dirty |= update(seq, "spectra_url", "")
    dirty |= update(seq, "local_spectra_file", "")
    dirty |= update(seq, "study", "ibd")
    dirty |= update(seq, "title", conf.study.mixs.project_name)


    seq._dirty = dirty
    seq.links['sequenced_from'] = [prep.id]
    save(seq, True)

    trms = list(seq.trimmed_seq_sets())
    if trms:
        trm = trms[0]
    else:
        trm = cutlass.SixteenSTrimmedSeqSet()
    dirty = False
    dirty |= update(trm, "comment", str("Trimmed 16S sample "+row[1]))
    dirty |= update_file(trm, seq.local_file, md5sum)
    dirty |= update(trm, "checksums", {"md5": md5sum})
    dirty |= update(trm, "format", "fastq")
    dirty |= update(trm, "format_doc", "https://en.wikipedia.org/wiki/FASTQ_format")
    dirty |= update(trm, "size", os.stat(seq.local_file).st_size)
    dirty |= update(trm, "study", "ibd")
    trm._dirty = dirty
    trm.links['computed_from'] = [seq.id]
    save(trm, True)

    abd_fname = output_dirs.amplicon.taxprof+"/"+row[1]+".biom"
    if not os.path.exists(abd_fname):
        return

    mtxs = list(trm.abundance_matrices())
    if mtxs:
        mtx = mtxs[0]
    else:
        mtx = cutlass.AbundanceMatrix()
    dirty = False
    md5sum = md5(abd_fname)
    dirty |= update_file(mtx, str(abd_fname), md5sum)
    dirty |= update(mtx, "checksums", {"md5": md5sum})    
    dirty |= update(mtx, "format", "biom")
    dirty |= update(mtx, "format_doc", "http://biom-format.org/#Biomformat-1.3.1 ")
    dirty |= update(mtx, "matrix_type", "16s_community")
    dirty |= update(mtx, "size", os.stat(mtx.local_file).st_size)
    dirty |= update(mtx, "study", "ibd")
    dirty |= update(mtx, "comment", "16S taxonomic profile "+str(row[1]))
    
    mtx.links['computed_from'] = [trm.id]
    save(mtx, True)
        

def datatype(row):
    return row[4]

def main():
    logging.basicConfig(format="%(asctime)s %(levelname)s: %(message)s")
    logging.getLogger().setLevel(logging.DEBUG)
    data, agg = aggregate(str(metadata_json))
    session = iHMPSession(conf.session.user, conf.session.passwd)
    pr = project(session)
    st = study(session, pr)
    save(st)
    dcc_subjs, dcc_vs, dcc_samps = dccify(session, st, agg)
    for l in (dcc_subjs, dcc_vs, dcc_samps):
        for obj in l:
            save(obj)
    sample_grp = groupby(attrgetter("name"), dcc_samps)
    for row in data['data']:
        samp = sample_grp.get(limsconvert(row[1]), [None])[0]
        if not samp:
            continue
        if datatype(row) == "metatranscriptomics":
            save_mtx(samp, row)
        elif datatype(row) == "metagenomics":
            save_wgs(samp, row)
        elif datatype(row) == "amplicon":
            save_16s(samp, row)
        elif datatype(row) == "proteomics":
            save_proteomics(samp, row)
    

if __name__ == '__main__':
    main()
